---
phase: 21-stabilization
plan: 02
type: execute
wave: 2
depends_on: ["21-01"]
files_modified:
  - .planning/phases/21-stabilization/21-VERIFICATION-RESULTS.md
autonomous: false

must_haves:
  truths:
    - "All 31 human verification tests have been executed in a browser and each has a documented pass/fail result"
    - "Test results are recorded in a structured verification document with phase, test number, result, and notes"
    - "Any failures include specific reproduction steps and observed behavior"
  artifacts:
    - path: ".planning/phases/21-stabilization/21-VERIFICATION-RESULTS.md"
      provides: "Complete test execution results for all 31 tests"
      contains: "Phase 13"
    - path: ".planning/phases/21-stabilization/21-VERIFICATION-RESULTS.md"
      provides: "Test results covering all phases"
      contains: "Phase 20"
  key_links:
    - from: ".planning/phases/21-stabilization/21-VERIFICATION-RESULTS.md"
      to: ".planning/phases/21-stabilization/21-03-PLAN.md"
      via: "Failure list feeds into remediation plan"
      pattern: "FAIL"
---

<objective>
Execute all 31 pending human verification tests across Phases 13, 15, 16, 17, and 20 in a live browser, document pass/fail results, and produce a structured verification results document.

Purpose: STAB-01 requires every pending test to have a recorded outcome. This cannot be automated -- it requires a human clicking through the UI in a browser. The agent prepares the test checklist document, the human executes tests and reports results, then the agent records the final results.

Output: 21-VERIFICATION-RESULTS.md with pass/fail for all 31 tests.
</objective>

<execution_context>
@/Users/developer/.claude/get-shit-done/workflows/execute-plan.md
@/Users/developer/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/21-stabilization/21-RESEARCH.md
@.planning/phases/21-stabilization/21-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Prepare verification test checklist document</name>
  <files>.planning/phases/21-stabilization/21-VERIFICATION-RESULTS.md</files>
  <action>
Create `.planning/phases/21-stabilization/21-VERIFICATION-RESULTS.md` with frontmatter and a structured checklist for all 31 tests. Group by phase. For each test include:
- Phase and test number
- Test description (from the research inventory)
- Category
- Prerequisites (e.g., "clear localStorage", "start server", "valid auth token")
- Steps to execute
- Expected result
- Result column (to be filled: PASS / FAIL / SKIP)
- Notes column (to be filled by human)

Use this exact test inventory from the research:

**Phase 13: Agent Session Manager (6 tests)**
1. Start agent session with simple prompt (requires running server + valid CLAUDE_CODE_OAUTH_TOKEN)
2. Start concurrent session on same project -- expect rejection message
3. Click Cancel during streaming -- expect stream stops
4. Simulate network disconnect during streaming -- expect reconnection attempt
5. Send prompt with code generation request -- expect syntax highlighted output
6. Verify archived projects do NOT show AI Agent section

**Phase 15: Simple/Expert Mode (5 tests)**
1. Visual toggle interaction -- sections appear/disappear when switching modes
2. LocalStorage persistence -- mode persists across browser refresh
3. Route guard redirect -- /ai, /soul, /integrations redirect to /home in Simple mode
4. CollapsibleSection chevron and auto-expand behavior
5. Expert mode zero regression -- all features accessible in Expert mode

**Phase 16: GSD Command Registry (6 tests)**
1. Visual appearance and layout of GSD command buttons on project detail
2. Phase-based command filtering -- commands change based on project phase
3. Expert mode command visibility and Cmd+K palette opens/works
4. Pause button during active session and session recovery after pause
5. Per-project independence of pause state
6. Command palette keyboard navigation (arrow keys, Enter, Escape)

**Phase 17: Guided Wizards (6 tests)**
1. Onboarding wizard first-time user flow (clear localStorage `eluma-onboarding-completed` first)
2. Idea creation wizard multi-input types (text, voice, GitHub URL)
3. Idea-to-project wizard quality gate enforcement (cannot skip questions)
4. Project creation wizard GSD integration (answers carry through)
5. Language switching in wizards (test in both EN and DE)
6. Quality gate progress indicator visual states (0/6 through 6/6)

**Phase 20: SDK Feature Adoption (8 tests)**
1. Stop reason display -- visual verification (end_turn, max_turns visible in UI)
2. Stop reason display -- error cases (show error stop reason)
3. Stop reason display -- German translation (switch to DE, verify translated)
4. Session recovery after server restart (stop server, restart, check recovered banner)
5. Deterministic session IDs (same project always gets same session ID prefix)
6. Auth health check at startup (check server logs for auth status message)
7. Auth health check -- invalid token (set bad token, verify warning log)
8. Session persistence during graceful shutdown (SIGTERM, check sessions saved)

Include a "Pre-Test Setup" section at the top:
- Ensure server is running: `cd /Users/developer/eluma && npm run dev`
- Ensure CLAUDE_CODE_OAUTH_TOKEN is valid (check server startup logs)
- Open browser to http://localhost:3000
- Clear localStorage if testing onboarding (test 17-1)
- Have both English and German available for i18n tests

Include a "Summary" section at the bottom with:
- Total: __/31
- Passed: __
- Failed: __
- Skipped: __
- Failure list (to be filled)
  </action>
  <verify>
1. File exists: `ls .planning/phases/21-stabilization/21-VERIFICATION-RESULTS.md`
2. Contains all 31 tests: `grep -c '| [0-9]' .planning/phases/21-stabilization/21-VERIFICATION-RESULTS.md` should be >= 31
3. Has all 5 phase sections: `grep -c 'Phase 1[3567]\|Phase 20' .planning/phases/21-stabilization/21-VERIFICATION-RESULTS.md` should be 5
  </verify>
  <done>
21-VERIFICATION-RESULTS.md exists with all 31 tests listed, grouped by phase, with pre-test setup instructions, execution steps, expected results, and empty result/notes columns ready for human input.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Human executes all 31 verification tests</name>
  <files>.planning/phases/21-stabilization/21-VERIFICATION-RESULTS.md</files>
  <action>
CHECKPOINT: Human executes all 31 verification tests in a browser.

What was built: Verification checklist document at `.planning/phases/21-stabilization/21-VERIFICATION-RESULTS.md` with all 31 tests across Phases 13, 15, 16, 17, and 20. Bug fixes and i18n improvements from Plan 21-01 are already applied.

How to verify -- execute all 31 tests in a browser. For each test:
1. Follow the steps described in the checklist
2. Record PASS, FAIL, or SKIP in the Result column
3. For FAILs: describe what you observed vs. what was expected in the Notes column
4. For SKIPs: explain why (e.g., "no valid auth token available")

Quick path if time-limited:
- Priority 1: Phase 15 (mode toggle, 5 tests) and Phase 17 (wizards, 6 tests) -- these test the UX that 21-01 just fixed
- Priority 2: Phase 16 (GSD commands, 6 tests) -- tests command palette and navigation
- Priority 3: Phase 13 and Phase 20 (agent sessions, 14 tests) -- require valid auth token

After testing, report results by pasting the filled-in table or listing failures like:
FAIL 13-4: Network disconnect did not trigger reconnection
PASS all Phase 15 tests

Resume signal: Paste your test results (pass/fail for each test, with failure details). Type "all pass" if everything passed.
  </action>
  <verify>Human has reported pass/fail results for all 31 tests (or a subset with justification for skipped tests).</verify>
  <done>All 31 tests have a documented PASS, FAIL, or SKIP result reported by the human tester.</done>
</task>

<task type="auto">
  <name>Task 3: Record final verification results</name>
  <files>.planning/phases/21-stabilization/21-VERIFICATION-RESULTS.md</files>
  <action>
Based on the human's reported test results from Task 2:

1. Update the Result column for each test in 21-VERIFICATION-RESULTS.md with PASS, FAIL, or SKIP
2. Fill in the Notes column with any observations the human reported
3. Update the Summary section at the bottom with final counts
4. If there are FAILs, list them in the "Failure list" section with:
   - Test ID (e.g., "13-4")
   - Brief description of the failure
   - Observed behavior
   - Expected behavior

This document becomes the input for Plan 21-03 (remediation).
  </action>
  <verify>
1. Every test has a result (PASS/FAIL/SKIP): `grep -c 'PASS\|FAIL\|SKIP' .planning/phases/21-stabilization/21-VERIFICATION-RESULTS.md` should be >= 31
2. Summary section is filled in with totals
3. If failures exist, they are listed with observed vs expected behavior
  </verify>
  <done>
21-VERIFICATION-RESULTS.md is complete with all 31 test outcomes documented, summary counts filled in, and any failures described with enough detail for remediation in Plan 21-03.
  </done>
</task>

</tasks>

<verification>
1. All 31 tests have documented outcomes (PASS/FAIL/SKIP)
2. No test is left without a result
3. Failure descriptions include enough detail to reproduce and fix
4. Summary counts are accurate (total = passed + failed + skipped = 31)
</verification>

<success_criteria>
- 21-VERIFICATION-RESULTS.md exists with all 31 test results recorded
- Every test has exactly one of: PASS, FAIL, SKIP
- Failed tests have actionable descriptions (what happened vs. what should have happened)
- Summary section totals match individual results
</success_criteria>

<output>
After completion, create `.planning/phases/21-stabilization/21-02-SUMMARY.md`
</output>
