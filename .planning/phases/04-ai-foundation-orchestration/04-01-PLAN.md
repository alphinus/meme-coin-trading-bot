---
phase: 04-ai-foundation-orchestration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - shared/types/ai.ts
  - server/src/ai/config.ts
  - server/src/ai/providers.ts
  - server/src/ai/generate.ts
  - server/src/ai/cost/pricing.ts
  - server/src/ai/cost/tracker.ts
  - server/src/ai/cost/dashboard.ts
autonomous: true
user_setup:
  - service: anthropic
    why: "AI LLM provider for Claude models"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "console.anthropic.com -> API Keys -> Create Key"
  - service: openai
    why: "AI LLM provider for GPT models"
    env_vars:
      - name: OPENAI_API_KEY
        source: "platform.openai.com -> API Keys -> Create new secret key"

must_haves:
  truths:
    - "AI SDK generates text responses via a unified provider registry"
    - "Every AI call records token usage and cost to a JSONL ledger"
    - "Temperature and max output tokens are configurable per task type"
    - "Model selection is automatic by task type but supports override"
  artifacts:
    - path: "shared/types/ai.ts"
      provides: "AI-related shared types (AiMemberConfig, CostRecord, AiTaskType, ModelOverride, TriggerRule)"
      exports: ["AiMemberConfig", "CostRecord", "AiTaskType", "ModelOverride"]
    - path: "server/src/ai/providers.ts"
      provides: "Provider registry with Anthropic + OpenAI via createProviderRegistry"
      exports: ["aiRegistry", "getModelForTask", "TASK_MODEL_MAP"]
    - path: "server/src/ai/generate.ts"
      provides: "generateAiResponse wrapper with cost tracking and language instruction"
      exports: ["generateAiResponse"]
    - path: "server/src/ai/cost/tracker.ts"
      provides: "Cost recording to JSONL via appendEvent"
      exports: ["recordAiCost"]
    - path: "server/src/ai/cost/dashboard.ts"
      provides: "Cost aggregation queries (by project, model, month)"
      exports: ["getCostSummary"]
  key_links:
    - from: "server/src/ai/generate.ts"
      to: "server/src/ai/providers.ts"
      via: "aiRegistry.languageModel(modelId)"
      pattern: "aiRegistry\\.languageModel"
    - from: "server/src/ai/generate.ts"
      to: "server/src/ai/cost/tracker.ts"
      via: "recordAiCost after every generateText call"
      pattern: "recordAiCost"
    - from: "server/src/ai/cost/tracker.ts"
      to: "server/src/events/store.ts"
      via: "appendEvent with type ai.cost_recorded"
      pattern: "appendEvent.*ai\\.cost_recorded"
---

<objective>
Build the AI infrastructure foundation: shared types, multi-provider LLM registry, text generation wrapper with automatic cost tracking, and cost aggregation for the dashboard.

Purpose: Every other Phase 4 plan depends on these building blocks. The provider registry enables multi-provider AI (AI-06), the generate wrapper enforces temperature presets (AI-07) and language selection (AI-04, I18N-02), and the cost tracker provides the data layer for the cost dashboard (AI-09).

Output: Shared AI types, provider registry, generate wrapper, cost tracker, cost dashboard query module
</objective>

<execution_context>
@/Users/developer/.claude/get-shit-done/workflows/execute-plan.md
@/Users/developer/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-ai-foundation-orchestration/04-RESEARCH.md

# Key integration points
@eluma/server/src/events/store.ts
@eluma/shared/types/events.ts
@eluma/shared/types/models.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create shared AI types and provider registry with task-type routing</name>
  <files>
    shared/types/ai.ts
    server/src/ai/config.ts
    server/src/ai/providers.ts
    server/src/ai/cost/pricing.ts
  </files>
  <action>
1. Install AI SDK dependencies in server workspace:
   ```
   npm install ai @ai-sdk/anthropic @ai-sdk/openai -w server
   ```

2. Create `shared/types/ai.ts` with all Phase 4 shared types:
   - `AiTaskType` union: "notification" | "reflection" | "mediation" | "pattern_match" | "summary" | "catch_logic"
   - `AiMemberConfig` interface: teamId, name (string), character (string), language ("de"|"en"), faceEmoji (string), hasMessage (boolean), pendingMessageId? (string)
   - `CostRecord` interface: id, projectId, modelId, taskType, inputTokens, outputTokens, totalCost, currency, timestamp
   - `ModelOverride` interface: nodeId, modelId, overriddenBy, overriddenAt, reason? (string)
   - `AiDecisionOverride` interface: aiEventId, overrideReason, newDecision, overriddenBy, overriddenAt
   - `TriggerRuleType` union: "milestone" | "pattern" | "conflict" | "catch"
   - `AiGenerateOptions` interface: taskType, projectId, language ("de"|"en"), systemPrompt, userPrompt, temperature?, maxOutputTokens?, modelOverride?, correlationId?

3. Create `server/src/ai/config.ts` with configuration constants:
   - `TEMPERATURE_PRESETS`: Record<AiTaskType, number> with values from research (notification: 0.3, reflection: 0.5, mediation: 0.4, pattern_match: 0.2, summary: 0.3, catch_logic: 0.1)
   - `MAX_TOKENS_PRESETS`: Record<AiTaskType, number> (notification: 200, reflection: 800, mediation: 1500, pattern_match: 500, summary: 600, catch_logic: 400)
   - `AI_SYSTEM_USER_ID`: "ai-system" (well-known constant, not a real auth credential)
   - Export all for use by generate.ts and trigger engine

4. Create `server/src/ai/providers.ts`:
   - Import `createProviderRegistry`, `customProvider` from "ai"
   - Import `createAnthropic` from "@ai-sdk/anthropic", `createOpenAI` from "@ai-sdk/openai"
   - Create providers reading API keys from `process.env.ANTHROPIC_API_KEY` and `process.env.OPENAI_API_KEY`
   - Build `aiRegistry` using `createProviderRegistry` with `customProvider` wrapping each provider:
     - anthropic: fast=claude-haiku-4-5, balanced=claude-sonnet-4-5, powerful=claude-opus-4-5
     - openai: fast=gpt-4o-mini, balanced=gpt-4o, powerful=o3
   - Export `TASK_MODEL_MAP`: Record<string, string> mapping task types to "provider:tier" strings (notification->anthropic:fast, reflection->anthropic:balanced, mediation->anthropic:powerful, pattern_match->anthropic:balanced, summary->openai:fast, catch_logic->anthropic:balanced)
   - Export `getModelForTask(taskType: string, override?: string): string` that returns override or TASK_MODEL_MAP entry or "anthropic:balanced" default
   - Handle missing API keys gracefully: if key is undefined, log a warning but do not crash at import time. The registry should be created lazily or with a try-catch so the server starts even without keys configured.

5. Create `server/src/ai/cost/pricing.ts`:
   - Export `MODEL_PRICING`: Record<string, { inputPerMillion: number; outputPerMillion: number }> for known models
   - Include pricing for: claude-haiku-4-5, claude-sonnet-4-5, claude-opus-4-5, gpt-4o-mini, gpt-4o, o3
   - Use approximate 2026 pricing from research. These are configuration values, easy to update.
   - Also export via the "provider:tier" aliases (e.g., "anthropic:fast" maps to same pricing as claude-haiku-4-5)
  </action>
  <verify>
    Run `cd /Users/developer/eluma && npx tsc --build --noEmit` -- TypeScript compiles with no errors. Verify `node_modules/ai/package.json` exists in server workspace. Verify shared/types/ai.ts exports are importable from server code.
  </verify>
  <done>
    AI SDK packages installed. Shared AI types defined. Provider registry created with Anthropic+OpenAI providers and task-type routing. Pricing table populated. All compiles cleanly.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create generate wrapper with cost tracking and cost dashboard aggregation</name>
  <files>
    server/src/ai/generate.ts
    server/src/ai/cost/tracker.ts
    server/src/ai/cost/dashboard.ts
  </files>
  <action>
1. Create `server/src/ai/cost/tracker.ts`:
   - Import `appendEvent` from "../../events/store.js"
   - Import `MODEL_PRICING` from "./pricing.js"
   - Export `recordAiCost(projectId: string, modelId: string, taskType: string, usage: { inputTokens: number; outputTokens: number }, correlationId: string): void`
   - Calculate input/output costs from MODEL_PRICING lookup (fallback to 0 if model not in pricing table)
   - Call appendEvent with type "ai.cost_recorded", aggregateType "ai_cost", aggregateId = projectId, userId = AI_SYSTEM_USER_ID, data = { modelId, taskType, inputTokens, outputTokens, inputCost, outputCost, totalCost, currency: "USD" }

2. Create `server/src/ai/generate.ts`:
   - Import `generateText` from "ai"
   - Import `aiRegistry`, `getModelForTask` from "./providers.js"
   - Import `recordAiCost` from "./cost/tracker.js"
   - Import `appendEvent` from "../events/store.js"
   - Import `TEMPERATURE_PRESETS`, `MAX_TOKENS_PRESETS`, `AI_SYSTEM_USER_ID` from "./config.js"
   - Import `AiGenerateOptions` from "shared/types/ai.js"
   - Export `async function generateAiResponse(options: AiGenerateOptions)`:
     a. Resolve model: `getModelForTask(options.taskType, options.modelOverride)`
     b. Get model from registry: `aiRegistry.languageModel(modelId)`
     c. Resolve temperature from options or TEMPERATURE_PRESETS or 0.4 default
     d. Resolve maxOutputTokens from options or MAX_TOKENS_PRESETS or 500 default
     e. Build language instruction: language === "de" ? "Antworte auf Deutsch." : "Respond in English."
     f. Call `generateText({ model, system: systemPrompt + languageInstruction, prompt: userPrompt, temperature, maxOutputTokens })`
     g. Record cost via `recordAiCost` if usage exists
     h. Append "ai.response_generated" event with aggregateType "ai_member", aggregateId = projectId, data = { taskType, modelId, text, finishReason, inputTokens, outputTokens, language }
     i. Return { text, usage, finishReason, event }
   - IMPORTANT: Wrap the generateText call in try/catch. On error, log the error and re-throw with a descriptive message. Do NOT swallow errors silently.

3. Create `server/src/ai/cost/dashboard.ts`:
   - Import `readEvents` from "../../events/store.js"
   - Export interface `CostSummary`: totalCost, byProject (Record<string, number>), byModel (Record<string, number>), byMonth (Record<string, number> with "YYYY-MM" keys)
   - Export `async function getCostSummary(): Promise<CostSummary>` that reads all "ai_cost" events and aggregates them
   - Export `async function getCostSummaryForProject(projectId: string): Promise<CostSummary>` that filters to a single project
  </action>
  <verify>
    Run `cd /Users/developer/eluma && npx tsc --build --noEmit` -- TypeScript compiles with no errors. Verify generate.ts imports resolve. Verify cost/tracker.ts uses appendEvent correctly (type matches existing store API).
  </verify>
  <done>
    generateAiResponse wrapper calls AI SDK, records costs, emits events. Cost tracker appends to JSONL. Cost dashboard aggregates by project/model/month. All TypeScript compiles.
  </done>
</task>

</tasks>

<verification>
- `npx tsc --build --noEmit` passes from project root
- `shared/types/ai.ts` exports are importable from both server and client workspaces
- `server/src/ai/` directory contains: config.ts, providers.ts, generate.ts, cost/tracker.ts, cost/pricing.ts, cost/dashboard.ts
- Provider registry handles missing API keys without crashing
- Temperature and token presets match research values
</verification>

<success_criteria>
- AI SDK packages (ai, @ai-sdk/anthropic, @ai-sdk/openai) installed in server workspace
- Provider registry created with Anthropic + OpenAI providers and task-type routing
- generateAiResponse wrapper enforces temperature presets, language instruction, and max output tokens
- Every AI call records cost to JSONL via appendEvent
- Cost dashboard can aggregate costs by project, model, and month
- All TypeScript compiles cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/04-ai-foundation-orchestration/04-01-SUMMARY.md`
</output>
